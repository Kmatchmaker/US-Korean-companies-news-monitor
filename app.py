import streamlit as st
import feedparser
import urllib.parse
from datetime import datetime

# 1. ì„¤ì •: ì£¼ë³„ í•œê¸€/ì˜ì–´ ë§¤í•‘
STATES_INFO = {
    "Georgia": "ì¡°ì§€ì•„",
    "Alabama": "ì•¨ë¼ë°°ë§ˆ",
    "Tennessee": "í…Œë„¤ì‹œ",
    "South Carolina": "ì‚¬ìš°ìŠ¤ìºë¡¤ë¼ì´ë‚˜",
    "Florida": "í”Œë¡œë¦¬ë‹¤"
}

st.set_page_config(page_title="2026 éŸ“ ê¸°ì—… ë¯¸êµ­ ì§„ì¶œ ì‹¤ì‹œê°„ ë³´ë“œ", layout="wide")
st.title("ğŸ­ 2026ë…„ 2ì›” ë¯¸ ë™ë‚¨ë¶€ ì§„ì¶œ ê¸°ì—… ì •ë°€ ëª¨ë‹ˆí„°ë§")
st.caption(f"ê¸°ì¤€ì¼: {datetime.now().strftime('%Y-%m-%d')} | ê³¼ê±° ê¸°ì‚¬ í•„í„°ë§ í™œì„±í™”")

# --- ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ (ë‚ ì§œ ì—°ì‚°ì ê°•ì œ ì£¼ì…) ---
def fetch_precise_news(query, lang, gl):
    # 'after:2026-02-01'ì„ ë¶™ì—¬ 2025ë…„ ë‰´ìŠ¤ë¥¼ ë¬¼ë¦¬ì ìœ¼ë¡œ ì°¨ë‹¨í•©ë‹ˆë‹¤.
    final_query = f"{query} after:2026-02-01"
    encoded_query = urllib.parse.quote(final_query)
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl={lang}&gl={gl}&ceid={gl}:{lang}"
    feed = feedparser.parse(url)
    # ì™„ë²½í•œ ë‚ ì§œìˆœ ì •ë ¬
    return sorted(feed.entries, key=lambda x: x.get('published_parsed', (0,0,0,0,0,0,0,0,0)), reverse=True)

# --- ë³´ë“œ ì„¤ê³„ ---
tab_us, tab_kr = st.tabs(["ğŸ‡ºğŸ‡¸ ë¯¸êµ­ í˜„ì§€ ì˜¤í”¼ì…œ (Gov & Local Media)", "ğŸ‡°ğŸ‡· í•œêµ­ ì£¼ìš” ì–¸ë¡  (Korean Press)"])

# ë³´ë“œ A: ë¯¸êµ­ í˜„ì§€ ë‰´ìŠ¤
with tab_us:
    st.markdown("### ğŸ›ï¸ ì£¼ ì •ë¶€ ë° í˜„ì§€ ê²½ì œ ë§¤ì²´ ë¦¬í¬íŠ¸")
    cols = st.columns(len(STATES_INFO))
    for i, (en_name, ko_name) in enumerate(STATES_INFO.items()):
        with cols[i]:
            st.info(f"ğŸ“ {en_name}")
            # ë¯¸êµ­ ë‚´ í•œêµ­ ê¸°ì—…(South Korea/Korean)ì˜ ì§„ì¶œ(Investment/Plant/Jobs) ì†Œì‹
            q_us = f'"{en_name}" ("South Korea" OR Korean) (investment OR factory OR plant)'
            items = fetch_precise_news(q_us, "en-US", "US")
            
            if not items:
                st.write("2ì›” ì‹ ê·œ ì†Œì‹ ì—†ìŒ")
            for entry in items[:7]:
                with st.container(border=True):
                    st.caption(f"ğŸ“… {entry.published[:16]}")
                    st.markdown(f"**[{entry.title.split(' - ')[0]}]({entry.link})**")
                    st.caption(f"Source: {entry.source.title}")

# ë³´ë“œ B: í•œêµ­ ì–¸ë¡  ë‰´ìŠ¤
with tab_kr:
    st.markdown("### ğŸ—ï¸ í•œêµ­ ì–¸ë¡ ì‚¬ì˜ ë¯¸êµ­ í˜„ì§€ ë³´ë„")
    cols = st.columns(len(STATES_INFO))
    for i, (en_name, ko_name) in enumerate(STATES_INFO.items()):
        with cols[i]:
            st.success(f"ğŸ“ {ko_name}")
            # í•œê¸€ ì§€ëª… + ë¯¸êµ­ ì§„ì¶œ ê´€ë ¨ í‚¤ì›Œë“œ
            q_kr = f'{ko_name} "ë¯¸êµ­" (ì§„ì¶œ OR íˆ¬ì OR ê³µì¥ OR ì±„ìš©)'
            items = fetch_precise_news(q_kr, "ko", "KR")
            
            if not items:
                st.write("2ì›” ì‹ ê·œ ë³´ë„ ì—†ìŒ")
            for entry in items[:7]:
                with st.container(border=True):
                    st.caption(f"ğŸ“… {entry.published[:16]}")
                    st.markdown(f"**[{entry.title.split(' - ')[0]}]({entry.link})**")
                    st.caption(f"ì¶œì²˜: {entry.source.title}")
