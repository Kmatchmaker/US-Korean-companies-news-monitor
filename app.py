import streamlit as st
import feedparser
import urllib.parse
from datetime import datetime

# 1. ëŒ€ìƒ ì£¼ ì„¤ì •
STATES = {
    "Georgia": "ì¡°ì§€ì•„",
    "Alabama": "ì•¨ë¼ë°°ë§ˆ",
    "Tennessee": "í…Œë„¤ì‹œ",
    "South Carolina": "ì‚¬ìš°ìŠ¤ìºë¡¤ë¼ì´ë‚˜",
    "Florida": "í”Œë¡œë¦¬ë‹¤"
}

st.set_page_config(page_title="2026 éŸ“ ê¸°ì—… ë¯¸êµ­ ì§„ì¶œ ì‹¤ì‹œê°„ ë³´ë“œ", layout="wide")
st.title("ğŸ­ 2026ë…„ 2ì›” éŸ“ ê¸°ì—… ë¯¸êµ­ ì§„ì¶œÂ·ì‹ ê·œ íˆ¬ì ë¦¬í¬íŠ¸")

# --- ì •ë°€ í•„í„°ë§ í•¨ìˆ˜ ---
def fetch_top_tier_news(query, lang, gl):
    # 2026ë…„ 2ì›” 1ì¼ ì´í›„ ë°ì´í„° ê°•ì œ
    date_filter = "after:2026-02-01"
    full_query = f'{query} {date_filter}'
    encoded_query = urllib.parse.quote(full_query)
    
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl={lang}&gl={gl}&ceid={gl}:{lang}"
    feed = feedparser.parse(url)
    
    verified = []
    seen_links = set()
    
    for entry in feed.entries:
        # ì—°ë„ ê²€ì¦ (2026ë…„ë§Œ í†µê³¼)
        if entry.get('published_parsed') and entry.published_parsed.tm_year == 2026:
            if entry.link not in seen_links:
                verified.append(entry)
                seen_links.add(entry.link)
                
    return sorted(verified, key=lambda x: x.get('published_parsed'), reverse=True)

# --- í™”ë©´ êµ¬ì„± ---
tab_us, tab_kr = st.tabs(["ğŸ‡ºğŸ‡¸ ë¯¸êµ­ ì˜¤í”¼ì…œ ë‰´ìŠ¤ (Gov & Biz)", "ğŸ‡°ğŸ‡· í•œêµ­/ë™í¬ ë‰´ìŠ¤ (Press)"])

with tab_us:
    st.info("ğŸ’¡ ì£¼ ì •ë¶€(Governor's Office) ë°œí‘œ ë° ë¯¸êµ­ ê²½ì œ ì „ë¬¸ì§€ ë³´ë„")
    for en_name, ko_name in STATES.items():
        st.markdown(f"#### ğŸ“ {en_name} ({ko_name})")
        col1, col2 = st.columns(2)
        
        with col1:
            st.caption("ğŸ›ï¸ ì£¼ ì •ë¶€ ê³µì‹ ë°œí‘œ (Official Release)")
            # [í•µì‹¬] ì£¼ ì •ë¶€ ì‚¬ì´íŠ¸(.gov)ì—ì„œ ë°œí‘œí•œ í•œêµ­ ê¸°ì—… íˆ¬ì ì†Œì‹ 
            # ì˜ˆ: 2ì›” 4ì¼ ë™ì›ì˜¤í†  ì¡°ì§€ì•„ íˆ¬ì ë°œí‘œ ë“±
            q = f'site:.gov "{en_name}" "South Korea" investment'
            items = fetch_top_tier_news(q, "en-US", "US")
            for e in items[:5]:
                with st.container(border=True):
                    st.markdown(f"**[{e.title.split(' - ')[0]}]({e.link})**")
                    st.caption(f"ğŸ“… {e.published[:16]}")
        
        with col2:
            st.caption("ğŸ“° ì§€ì—­/ì „êµ­ ê²½ì œì§€ (Business Media)")
            # ì¡°ì§€ì•„ì˜ ê²½ìš° AJCë‚˜ Business Chronicle ë“± ìœ ë ¥ì§€ ì¤‘ì‹¬
            q = f'"{en_name}" "South Korea" investment (Journal OR Chronicle OR News)'
            items = fetch_top_tier_news(q, "en-US", "US")
            for e in items[:5]:
                with st.container(border=True):
                    st.markdown(f"**[{e.title.split(' - ')[0]}]({e.link})**")
                    st.caption(f"ğŸ“… {e.published[:16]} | {e.source.title}")

with tab_kr:
    st.success("ğŸ’¡ êµ­ë‚´ ê²½ì œì§€ ë° ë¯¸êµ­ í˜„ì§€ í•œì¸ ë§¤ì²´ ë³´ë„")
    for en_name, ko_name in STATES.items():
        st.markdown(f"#### ğŸ“ {ko_name} ({en_name})")
        col_main, col_dia = st.columns(2)
        
        with col_main:
            st.caption("ğŸ—ï¸ í•œêµ­ ì£¼ìš” ì–¸ë¡  (ë„¤ì´ë²„ ë‰´ìŠ¤ ë“±)")
            q = f'{ko_name} "ë¯¸êµ­" (íˆ¬ì OR ì§„ì¶œ OR ê³µì¥)'
            items = fetch_top_tier_news(q, "ko", "KR")
            for e in items[:7]:
                with st.container(border=True):
                    st.markdown(f"**[{e.title.split(' - ')[0]}]({e.link})**")
                    st.caption(f"ğŸ“… {e.published[:16]} | {e.source.title}")
                    
        with col_dia:
            st.caption("ğŸ‡ºğŸ‡¸ ë¯¸ í˜„ì§€ ë™í¬ ì‹ ë¬¸ (í•œì¸ ë‰´ìŠ¤)")
            q = f'{ko_name} "íˆ¬ì" OR "ê³µì¥" OR "ì§„ì¶œ"'
            items = fetch_top_tier_news(q, "ko", "US")
            for e in items[:7]:
                with st.container(border=True):
                    st.markdown(f"**[{e.title.split(' - ')[0]}]({e.link})**")
                    st.caption(f"ğŸ“… {e.published[:16]} | {e.source.title}")
