import streamlit as st
import feedparser
import urllib.parse
from datetime import datetime

# ì„¤ì •: 5ê°œ ì£¼
STATES = ["Georgia", "Alabama", "Tennessee", "South Carolina", "Florida"]

st.set_page_config(page_title="2026 ë¯¸ ë™ë‚¨ë¶€ ê¸°ì—… íˆ¬ì ëª¨ë‹ˆí„°", layout="wide")
st.title("ğŸ“Š ë¯¸ ë™ë‚¨ë¶€ í•œêµ­ ê¸°ì—… ì§„ì¶œ ì‹¤ì‹œê°„ ë³´ë“œ (2026)")

# --- ë‰´ìŠ¤ ìˆ˜ì§‘ í•¨ìˆ˜ ---
def fetch_news(query, lang="en-US", gl="US"):
    # 2026ë…„ ìµœì‹ ì„± ë³´ì¥ì„ ìœ„í•´ when:30d í•„í„° ìœ ì§€
    encoded_query = urllib.parse.quote(f"{query} when:30d")
    url = f"https://news.google.com/rss/search?q={encoded_query}&hl={lang}&gl={gl}&ceid={gl}:{lang}"
    feed = feedparser.parse(url)
    # ë°œí–‰ ì‹œê°„ìˆœ ì •ë ¬
    entries = sorted(feed.entries, key=lambda x: x.get('published_parsed', (0,0,0,0,0,0,0,0,0)), reverse=True)
    return entries

# --- í™”ë©´ ë ˆì´ì•„ì›ƒ (Tabs) ---
tab_us, tab_kr = st.tabs(["ğŸ‡ºğŸ‡¸ ë¯¸êµ­ í˜„ì§€ ë‰´ìŠ¤ (US Media & Gov)", "ğŸ‡°ğŸ‡· í•œêµ­ ì–¸ë¡  ë³´ë„ (Korean Media)"])

# --- TAB 1: ë¯¸êµ­ í˜„ì§€ ë³´ë„ (í•„í„° ìˆ˜ì • ì™„ë£Œ) ---
with tab_us:
    st.info("ğŸ’¡ ë¯¸êµ­ í˜„ì§€ ë§¤ì²´ê°€ ë³´ë„í•œ 'í•œêµ­ ê¸°ì—…'ì˜ ì§„ì¶œ ë° íˆ¬ì ë‰´ìŠ¤ì…ë‹ˆë‹¤.")
    cols = st.columns(len(STATES))
    for i, state in enumerate(STATES):
        with cols[i]:
            st.header(f"ğŸ“ {state}")
            # [ìˆ˜ì •] ì£¼ ì´ë¦„ê³¼ 'South Korea'ë¥¼ í•„ìˆ˜ ê²°í•©í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ í‚¤ì›Œë“œ ì¶”ê°€
            # ë‹¨ìˆœ ì£¼ ì •ë¶€ ì‚¬ì´íŠ¸ ê²€ìƒ‰ì„ ë„˜ì–´ í˜„ì§€ ê²½ì œì§€ë„ í¬í•¨í•˜ë„ë¡ í™•ì¥
            query = f'"{state}" "South Korea" (investment OR factory OR plant OR jobs)'
            news_items = fetch_news(query, lang="en-US", gl="US")
            
            if not news_items:
                st.write("í•´ë‹¹ ì£¼ì˜ ì‹ ê·œ ê¸°ì—… ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            for entry in news_items[:8]:
                with st.container(border=True):
                    # ì œëª©ì—ì„œ ì£¼ ì´ë¦„ì´ ìˆëŠ”ì§€ ì¬ê²€ì¦í•˜ì—¬ ë°ì´í„° ì„ì„ ë°©ì§€
                    if state.lower() in entry.title.lower() or state.lower().replace(" ", "") in entry.link.lower():
                        st.caption(f"ğŸ“… {entry.published[:16]} | {entry.source.title}")
                        st.markdown(f"**[{entry.title.split(' - ')[0]}]({entry.link})**")
                        if 'summary' in entry:
                            st.write(f"ğŸ“ {entry.summary.split('<')[0][:120]}...")

# --- TAB 2: í•œêµ­ ì–¸ë¡  ë³´ë„ ---
with tab_kr:
    st.success("ğŸ’¡ í•œêµ­ ë‚´ ì–¸ë¡ ì‚¬ê°€ ë³´ë„í•œ ë¯¸êµ­ í˜„ì§€ ì§„ì¶œ ì†Œì‹ì…ë‹ˆë‹¤.")
    cols = st.columns(len(STATES))
    for i, state in enumerate(STATES):
        with cols[i]:
            st.header(f"ğŸ“ {state}")
            # í•œêµ­ì–´ í‚¤ì›Œë“œ ì •ë°€í™”
            query = f'"{state}" "í•œêµ­ ê¸°ì—…" (íˆ¬ì OR ì§„ì¶œ OR ê³µì¥ OR ì±„ìš©)'
            news_items = fetch_news(query, lang="ko", gl="KR")
            
            if not news_items:
                st.write("ê´€ë ¨ ë³´ë„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            for entry in news_items[:8]:
                with st.container(border=True):
                    st.caption(f"ğŸ“… {entry.published[:16]} | {entry.source.title}")
                    st.markdown(f"**[{entry.title.split(' - ')[0]}]({entry.link})**")
                    if 'summary' in entry:
                        st.write(f"ğŸ“ {entry.summary.split('<')[0][:120]}...")
