import streamlit as st
import feedparser
import urllib.parse
from datetime import datetime

# 1. ì£¼ë³„ ìµœì í™”ëœ ì •ë°€ ì¿¼ë¦¬ (ë…¸ì´ì¦ˆ ì œê±° ê°•í™”)
# íŠ¹íˆ AL, SC, FLì€ ì‚°ì—… í‚¤ì›Œë“œë¥¼ í•„ìˆ˜ í¬í•¨(+)í•˜ë„ë¡ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤.
STATE_QUERIES = {
    "Georgia": 'Georgia ("South Korea" OR Korean) (investment OR factory OR Dongwon) when:30d',
    "Alabama": 'Alabama ("South Korea" OR Korean) (Hyundai OR factory OR investment OR automotive) when:30d',
    "Tennessee": 'Tennessee ("South Korea" OR Korean) (LG OR SK OR investment OR factory) when:30d',
    "South Carolina": '"South Carolina" ("South Korea" OR Korean) (Samsung OR factory OR investment OR manufacturing) when:30d',
    "Florida": 'Florida ("South Korea" OR Korean) (investment OR "new office" OR energy OR technology) when:30d'
}

st.set_page_config(page_title="2026 ë¯¸ ë™ë‚¨ë¶€ ê¸°ì—… ëª¨ë‹ˆí„°", layout="wide")
st.title("ğŸšœ 2026ë…„ ë¯¸ ë™ë‚¨ë¶€ éŸ“ ê¸°ì—… ì§„ì¶œ ì •ë°€ ë¦¬í¬íŠ¸")

def fetch_and_clean_news(state_name):
    query = STATE_QUERIES.get(state_name)
    encoded_query = urllib.parse.quote(query)
    
    # ê¸€ë¡œë²Œ(US) ë‰´ìŠ¤ë¥¼ ë©”ì¸ìœ¼ë¡œ í•˜ë˜, í•œêµ­ì–´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë³´ì¡°ë¡œ í•©ì¹©ë‹ˆë‹¤.
    news_feeds = [
        f"https://news.google.com/rss/search?q={encoded_query}&hl=en-US&gl=US&ceid=US:en",
        f"https://news.google.com/rss/search?q={encoded_query}&hl=ko&gl=KR&ceid=KR:ko"
    ]
    
    all_entries = []
    for url in news_feeds:
        feed = feedparser.parse(url)
        all_entries.extend(feed.entries)
    
    # ì‹œê°„ìˆœ ì •ë ¬
    all_entries.sort(key=lambda x: x.get('published_parsed', (0,0,0,0,0,0,0,0,0)), reverse=True)
    
    unique_news = []
    seen_titles = set()
    
    # 2ë‹¨ê³„ í•„í„°: ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ 'íˆ¬ì/ê³µì¥/ê¸°ì—…' ê´€ë ¨ ë‹¨ì–´ê°€ ì œëª©ì— ìˆëŠ” ê²ƒë§Œ ìŠ¹ì¸
    biz_keywords = ["invest", "factory", "plant", "company", "jobs", "new", "open", "korean", "korea", "venture", "partnership"]

    for entry in all_entries:
        title_lower = entry.title.lower()
        pure_title = entry.title.split(' - ')[0].strip()
        
        # ì£¼ ì´ë¦„ ê²€ì¦ + ë¹„ì¦ˆë‹ˆìŠ¤ í‚¤ì›Œë“œ ê²€ì¦
        if state_name.lower() in title_lower:
            if any(kb in title_lower for kb in biz_keywords):
                if pure_title not in seen_titles:
                    # 2024ë…„ ì´ì „ êµ¬í˜• ë‰´ìŠ¤ ê°•ì œ ë°°ì œ
                    if "2024" not in entry.title and "2023" not in entry.title:
                        unique_news.append(entry)
                        seen_titles.add(pure_title)
                
    return unique_news[:8]

# ëŒ€ì‹œë³´ë“œ í™”ë©´ êµ¬ì„±
cols = st.columns(len(STATE_QUERIES))

for i, state in enumerate(STATE_QUERIES.keys()):
    with cols[i]:
        st.subheader(f"ğŸ“ {state}")
        news_items = fetch_and_clean_news(state)
        
        if not news_items:
            st.info("í˜„ì¬ ê´€ë ¨ ë¹„ì¦ˆë‹ˆìŠ¤ ì†Œì‹ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        for entry in news_items:
            with st.container(border=True):
                st.caption(f"ğŸ“… {entry.published[:16]} | {entry.source.title}")
                st.markdown(f"#### [{entry.title.split(' - ')[0]}]({entry.link})")
                
                if 'summary' in entry:
                    clean_summary = entry.summary.split('<')[0]
                    if len(clean_summary) > 10:
                        st.write(f"ğŸ“ {clean_summary[:140]}...")
